## Statistical Modeling with `statsmodels`

This section was written by Leon Nguyen.

### Introduction

Hello! My name is Leon Nguyen (they/she) and I am a second-year undergraduate 
student studying Statistical Data Science and Mathematics at the University 
of Connecticut, aiming to graduate in Fall 2025. One of my long-term goals is to 
make the field of data science more accessible to marginalized communities and 
minority demographics. My research interests include statistical modeling, 
data visualization, and design. Statistical modeling is one of the most 
fundamental skills required for data science, and it's important to have a solid 
understanding of how models work for interpretable results. 

The `statsmodels` Python package offers a diverse range of classes and functions 
tailored for estimating various statistical models, conducting statistical tests, 
and exploring statistical data. Each estimator provides an extensive array of 
result statistics, rigorously tested against established statistical packages to 
ensure accuracy. This presentation will focus on the practical applications of the 
statistical modeling aspect.

### Key Features and Capabilities

Some key features and capabilities of `statsmodels` are:

+ Generalized Linear Models
+ Nonparametric methods
+ Diagnostic Tests

In this presentation, we will work with practical applications of statistical 
modeling in `statsmodels`. We will briefly cover how to set up linear, logistic, 
and Poisson regression models, and touch upon kernel density estimation and 
diagnostics. By the end of this presentation, you should be able to understand how 
to use `statsmodels` to analyze your own datasets using these fundamental 
techniques.

### Installation and Setup

To install `statsmodels`, use the following code, depending on whether you are 
using pip or conda: 
```{python}
%pip install statsmodels
```

```{python}
# %conda install statsmodels
```

One of the major benefits of using `statsmodels` is their compatability with 
other commnonly used packages, such as `NumPy`, `SciPy`, and `Pandas`. 
These packages provide foundational scientific computing functionalities that  
are crucial for working with `statsmodels`. To ensure everything is set up  
correctly, import the necessary libraries at the beginning of your script:

```{python}
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import random as rd     # for test and training data splitting
import matplotlib.pyplot as plt
```

Here are some minimum dependencies:

+ Python >= 3.8
+ NumPy >= 1.18
+ SciPy >= 1.4
+ Pandas >= 1.0
+ Patsy >= 0.5.2

The last item listed above, `patsy`, is a Python library that provides a simple 
and expressive syntax for specifying statistical models in Python. It allows 
users to define linear models using a formula syntax similar to the formulas 
used in R and other statistical software. More `patsy` documentation can be 
found [here](https://patsy.readthedocs.io/en/latest/).

Cython is required to build from a git checkout but not to run or install from 
PyPI. The documentation states this required to build the code from github but 
not from a source distribution:

+ Cython >= 0.29.33 

### Importing Data

There are a few different options to import data. For example, `statsmodels` 
documentation demonstrates how to importing from a CSV file hosted online from 
the [Rdatasets repository](https://github.com/vincentarelbundock/Rdatasets/):

```{python}
# Reads the 'Guerry' dataset from the HistData package into df
df = sm.datasets.get_rdataset("Guerry", "HistData").data

# Print out the first five rows of our dataframe
print(df.head())
```

We can also read directly from a local CSV file with `pandas`. For example, we will 
be using the NYC 311 rodent data:
```{python}
# Reads the csv file into df
df = pd.read_csv('data\\rodent_2022-2023.csv')

# Brief data pre-processing
df['Created Date'] = pd.to_datetime(df['Created Date'], format = "%m/%d/%Y %I:%M:%S %p")
df['Closed Date'] = pd.to_datetime(df['Closed Date'], format = "%m/%d/%Y %I:%M:%S %p")
df['Created Year'] = df['Created Date'].dt.year
df['Created Month'] = df['Created Date'].dt.month
df['Response Time'] = df['Closed Date'] - df['Created Date'] 
df['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600) # in hours

# Print out the first five rows of our dataframe
print(df.head())
```


### Troubleshooting

Whenever you are having problems with `statsmodels`, you can access the  
official documentation by visiting [this link](https://www.statsmodels.org/stable/index.html). If you are working in a code editor, you can also run the following in a code cell:
```{python}
sm.webdoc() 
# Opens the official documentation page in your browser
```

To look for specific documentation, for example `sm.GLS`, you can run the following:
```{python}
sm.webdoc(sm.GLS, stable=True)
# First argument (func) : string* or function to search for documentation 
# Second argument (stable) : (True) or development (False) documentation, default is stable

# *Searching via string has presented issues?
```

### Statistical Modeling and Analysis

Constructing statistical models with `statsmodels` generally follows a step-by-step 
process: 

1. **Import necessary libraries**: This includes both `numpy` and `pandas`, as well 
as `statsmodels.api` itself (`sm`).

1. **Load** the data: This could be data from the `rdataset` repository, local 
csv files, or other formats. In general, it's best practice to load your data 
into a `pandas` DataFrame so that it can easily be manipulated using `pandas` 
functions.

1. **Prepare the data**: This involves converting variables into appropriate 
types (e.g., categorical into factors), handling missing values, and creating 
appropriate interaction terms.

1. **Define** our model: what model is the appropriate representation of our 
research question? This could be an OLS regression (`sm.OLS`), logistic 
regression (`sm.Logit`), or any number of other models depending on the 
nature of our data.
   
1. **Fit** the model to our data: we use the `.fit()` method which takes as input 
our dependent variable and independent variables.
   
1. **Analyze** the results of the model: this is where we can get things like 
parameter estimates, standard errors, p-values, etc. We use the `.summary()` 
method to print out these statistics.


### Generalized Linear Models

GLM models allow us to construct a linear relationship between the response and 
predictors, even if their underlying relationship is not linear. This is done via 
a link function, which is a transformation which links the response variable to a 
linear model. 

Key points of GLMs:

+ Data should be independent and random.
+ The response variable $Y$ does not need to be normally distributed, but the 
distribution is from an exponential family (e.g. binomial, Poisson, normal).
+ GLMs do not assume a linear relationship between the response variable and 
the explanatory variables, but assume a linear relationship between 
the transformed expected response in terms of the link function and the 
explanatory variables.
+ GLMs are useful when the range of your response variable is constrained 
and/or the variance is not constant or normally distributed. 
+ GLM models transform the response variable to allow the fit to be done by 
least squares. The transformation done on the response variable is defined by 
the link function.

#### Linear Regression

Simple and muliple linear regression are special cases where the expected value 
of the dependent value is equal to a linear combination of predictors. In other 
words, the link function is the identity function $g[E(Y)]=E(Y)$. Make sure 
assumptions for linear regression hold before proceeding. The model for 
linear regression is given by:
$$y_i = X_i'\beta + \epsilon_i$$
where $X_i$ is a vector of predictors for individual $i$, and $\beta$ is a vector of coefficients that define this linear combination.

Here is an application of SLR with `statsmodels`:
```{python}
# We can use .get_rdataset() to load data into Python from a repositiory of R packages.
df1 = sm.datasets.get_rdataset('avocado', package="causaldata").data

# Fit regression model
results1 = smf.ols('TotalVolume ~ AveragePrice', data=df1).fit()

# Inspect the results
print(results1.summary())
```


#### Logistic Regression

Logistic regression is used when the response variable is binary. The 
response distribution is logistic which means it has support on $(0,1)$ and  
is invertible. The log-odds link function is defined as $\log\left(\frac{p}{1-p}\right)$,  
where $p$ is the predicted probability.

Here we have an example from `rodents` dataset, where the response variable 
`Under 3h` indicates whether the response time for an observation was under 3 hours. 
1 indicates that the response time is less than 3 hours, and 0 indications greater 
than or equal to 3 hours.

```{python}
# Loaded the dataset in a previous cell

# 
df['Under 3h'] = (df['Response Time'] < 3).astype(int)

# Convert the categorical variable to dummy variables
df = df.loc[:, ['Borough', 'Open Data Channel Type', 'Under 3h']]
df = pd.get_dummies(df, dtype = int)
```

```{python}
# Fill NaN values with mean
df.fillna(df.mean(), inplace = True)

# Use data for training
rd.seed(3255)
training_df = df.sample(frac = 0.8, random_state = 0)

# Use remaining data for testing 
test_df = df.drop(training_df.index)   

# Fit the logistic regression model using statsmodels
endog_train = training_df['Under 3h']
exog_train = training_df.drop(columns = 'Under 3h', axis=1)

logitmod = sm.Logit(endog_train, exog_train)
result = logitmod.fit(maxiter=30)
# Summary of the fitted model
print(result.summary())

```

We can use the test data for predictions with our model:
```{python}
endog_test = test_df['Under 3h']
exog_test = test_df.drop(columns = 'Under 3h', axis=1)

predictions = logitmod.predict(result.params, exog=exog_test)
predictions
```

Let's evaluate how well our model performs by calculating some metrics:

```{python}
# from sklearn.metrics import classification_report, confusion_matrix

# y_true = test_df["Under 3h"]
# y_pred = predictions

# # Confusion matrix 
# cm = confusion_matrix(y_true, y_pred)  
# print ("Confusion Matrix : \n", cm)  
  
# # Accuracy score of the model 
# print('Test accuracy = ', accuracy_score(test_df['over3d'], prediction))
```

Another example:

We will use the `macrodata` dataset directly from `statsmodels`, which contains 
information on macroeconomic indicators such as unemployment rate. I have created a 
binary variable `morethan5p` that has a value of 1 when the unemployment rate is 
more than 5% in a given observation, and is 0 when it is equal to or less than 5%. 

```{python}
# df2 is an instance of a statsmodels dataset class
df2 = sm.datasets.macrodata.load_pandas()
# add binary variable
df2.data['morethan5p'] = (df2.data['unemp']>5).apply(lambda x:int(x))
# Subset data
df2 = df2.data[['morethan5p','cpi','pop']]
```

```{python}
# Data splitting for model training and prediction
rd.seed(3255)
df2_train = df2.sample(frac = 0.8, random_state = 0)
df2_test = df2.drop(df2_train.index)

# Logit regression model
model = smf.logit("morethan5p ~ cpi + pop", df2_train)
results = model.fit()
summary = results.summary()
print(summary)
```

How to interpret the summary chart:

+ **coef:** the coefficients of the independent variables in the regression equation.
+ **Log-Likelihood:** the natural logarithm of the Maximum Likelihood Estimation(MLE) function. MLE is the optimization process of finding the set of parameters that result in the best fit.
+ **LL-Null:** the value of log-likelihood of the model when no independent variable is included(only an intercept is included).
+ **Pseudo R-squ.:** a substitute for the R-squared value in Least Squares linear regression. It is the ratio of the log-likelihood of the null model to that of the full model.

We can compute odds ratios and other infomration by calling methods on the fitted result object:
```{python}
odds_ratios = pd.DataFrame(
    {
        "Odds Ratio": results.params,
        "Lower CI": results.conf_int()[0],
        "Upper CI": results.conf_int()[1],
    }
)
odds_ratios = np.exp(odds_ratios)

print(odds_ratios)
```

We can also run predictions on this model:

```{python}
# Prediction on train set
prediction_train = results.predict(df2_test[['cpi', 'pop']])
prediction_train
```

```{python}
# from sklearn.metrics import classification_report, confusion_matrix

# y_true = df2_test['morethan5p']
# y_pred = prediction_train

# # Confusion matrix 
# cm = confusion_matrix(y_true, y_pred)  
# print ("Confusion Matrix : \n", cm)  
  
# # Accuracy score of the model 
# print('Test accuracy = ', accuracy_score(test_df['over3d'], prediction))
```

There appears to be some ambiguity how the data is handled when predicting accuracy, so this may need further clarification.

#### Poisson Regression

This type of regression is best suited for modeling the how the mean of a discrete 
variable depends on one or more predictors.

The log of the probability of success is modeled by:

$\log(p) = b_0 + b_1x_1 + ... + b_kx_k$

where p is the probability of success (the response variable). The intercept `b0` is     
assumed to be 0 if not provided in the model. We will use `.add_constant` to indicate 
that our model includes an intercept term.

Let's use the `sunspots` dataset from `statsmodels`. We first load an instance of a 
`statsmodels` dataset class, analogous to a `pandas` dataframe:
```{python}
df3 = sm.datasets.sunspots.load_pandas()
df3 = df3.data
df3.head()

X = sm.add_constant(df3['YEAR']) 
# .add_constant indicates that our model includes an intercept term
Y = df3['SUNACTIVITY']
```

We can use the `.GLM` function with the `family='poisson'` argument to fit our 
model. Some important parameters:

+ `data.endog` acts as a series of observations for the dependent variable $Y$
+ `data.exog` acts as a series of observations for each predictor
+ `family` specifies the distribution appropriate for the model

```{python}
results = sm.GLM(Y, X, family=sm.families.Poisson()).fit()
print(results.summary())
```


### Nonparametric Models

#### Kernel Density Estimation

`statsmodels` has a non-parametric approach called kernel density estimation (KDE), 
which estimates the underlying probability of a given assortment of data points. 
KDE is used when you don't have enough data points to form a parametric model. 
It estimates the density of continuous random variables, or extrapolates some 
continuous function from discrete counts. KDE is a non-parametric way to estimate 
the underlying distribution of data. The KDE weights all the distances of all data 
points relative to every location. The more data points there are at a given 
location, the higher the KDE estimate at that location. Points closer to a given 
location are generally weighted more than those further away. The shape of the 
kernel function itself indicates how the point distances are weighted:

```{python}
from statsmodels.nonparametric.kde import kernel_switch
list(kernel_switch.keys())

# Create a figure
fig = plt.figure(figsize=(12, 5))

# Enumerate every option for the kernel
for i, (ker_name, ker_class) in enumerate(kernel_switch.items()):

    # Initialize the kernel object
    kernel = ker_class()

    # Sample from the domain
    domain = kernel.domain or [-3, 3]
    x_vals = np.linspace(*domain, num=2 ** 10)
    y_vals = kernel(x_vals)

    # Create a subplot, set the title
    ax = fig.add_subplot(3, 3, i + 1)
    ax.set_title('Kernel function "{}"'.format(ker_name))
    ax.plot(x_vals, y_vals, lw=3, label="{}".format(ker_name))
    ax.scatter([0], [0], marker="x", color="red")
    plt.grid(True, zorder=-5)
    ax.set_xlim(domain)

plt.tight_layout()
```

KDE can be applied for univariate or multivariate data. `statsmodels` has two  methods for this:
- `sm.nonparametric.KDEunivariate`: For univariate data. This estimates the 
bandwidth using Scott’s rule unless specified otherwise. Much faster than 
using `.KDEMultivariate` due to its use of Fast Fourier Transforms on 
univariate, continuous data.
- `sm.nonparametric.KDEMultivariate`: This applies to both univariate and 
multivariate data, but tends to be slower. Can use mixed types of data but requires specification.

Here we will demonstrate how to apply it to univariate data, based off of 
examples provided in the [documentation](https://www.statsmodels.org/stable/examples/notebooks/generated/kernel_density.html#Comparing-kernel-functions). 
We will generate a histogram of random data. Our goal is to fit a KDE with a Gaussian kernal function to this data.

```{python}
from statsmodels.distributions.mixture_rvs import mixture_rvs
from scipy import stats

# Location, scale and weight for the two distributions
dist1_loc, dist1_scale, weight1 = -1, 0.5, 0.25
dist2_loc, dist2_scale, weight2 = 1, 0.5, 0.75

# Sample from a mixture of distributions
obs_dist = mixture_rvs(
    prob=[weight1, weight2],
    size=250,
    dist=[stats.norm, stats.norm],
    kwargs=(
        dict(loc=dist1_loc, scale=dist1_scale),
        dict(loc=dist2_loc, scale=dist2_scale),
    ),
)

fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111)

# Scatter plot of data samples and histogram
ax.scatter(
    obs_dist,
    np.abs(np.random.randn(obs_dist.size)),
    zorder=15,
    color="orange",
    marker="o",
    alpha=0.5,
    label="Samples",
)
lines = ax.hist(obs_dist, bins=25, edgecolor="w", label="Histogram")

ax.legend(loc="best")
ax.grid(True, zorder=-5)
```

Now we want to fit our KDE based on our `obs_dist` sample:
```{python}
kde = sm.nonparametric.KDEUnivariate(obs_dist)
kde.fit()  # Estimate the densities
print("Estimated Bandwidth:", kde.bw)  
# Print the bandwidth (also called kernel size or smoothing parameter)
```

```{python}
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111)

# Plot the histogram
ax.hist(
    obs_dist,
    bins=25,
    density=True,
    label="Histogram from samples",
    zorder=5,
    edgecolor="w",
    alpha=0.8,
)

# Plot the KDE as fitted using the default arguments
ax.plot(kde.support, kde.density, lw=3, label="KDE from samples", zorder=10, color='red', alpha=0.8)

# Plot the true distribution which we artificially generated
true_values = (
    stats.norm.pdf(loc=dist1_loc, scale=dist1_scale, x=kde.support) * weight1
    + stats.norm.pdf(loc=dist2_loc, scale=dist2_scale, x=kde.support) * weight2
)

ax.plot(kde.support, true_values, lw=3, label="True distribution", zorder=15, color='lime', alpha=0.8)

# Plot the samples
ax.scatter(
    obs_dist,
    np.abs(np.random.randn(obs_dist.size)) / 40,
    marker="o",
    color="orange",
    zorder=20,
    label="Samples",
    alpha=0.5,
)

ax.legend(loc="best")
ax.grid(True, zorder=-5)
```

We can alter the `bw` or bandwidth parameter of the KDE to see how it affects the 
fit and smoothness of the curve. The smaller the bandwidth, the more jagged the 
estimated distribution becomes.

```{python}

fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111)

# Plot the histogram
ax.hist(
    obs_dist,
    bins=25,
    density=True,
    label="Histogram from samples",
    zorder=5,
    edgecolor="w",
    alpha=0.8,
)

# Plot the true distribution which we artificially generated
true_values = (
    stats.norm.pdf(loc=dist1_loc, scale=dist1_scale, x=kde.support) * weight1
    + stats.norm.pdf(loc=dist2_loc, scale=dist2_scale, x=kde.support) * weight2
)
ax.plot(kde.support, true_values, lw=3, label="True distribution", zorder=15, color='lime', alpha=0.7)

# Plot the KDE for various bandwidths
for bandwidth in [0.1, 0.2, 0.4]:
    kde.fit(bw=bandwidth)  # Estimate the densities
    ax.plot(
        kde.support,
        kde.density,
        "--",
        lw=2,
        color="r",
        zorder=10,
        label="KDE from samples, bw = {}".format(round(bandwidth, 2)),
    )

# Plot the samples
ax.scatter(
    obs_dist,
    np.abs(np.random.randn(obs_dist.size)) / 40,
    marker="o",
    color="orange",
    zorder=20,
    label="Samples",
    alpha=0.5,
)

ax.legend(loc="best")
ax.grid(True, zorder=-5)
```

### Diagnostic Tests

The diagnostics tests allow us to check if our model assumptions hold true. One 
of the most important assumption of GLMs is that the expected value of the residuals 
is zero. Another key assumption is that there are no perfect multicollinearity between predictors. We can check these assumptions using the following diagnostic tests: 

- **Residual Plots**: Residual plots help identify structure in the data that 
might introduce bias into the estimates. Residuals should look like white noise. 
If they don’t, we have some structure in our data which may indicate collinearity.


```{python}
```


### Conclusion


### References

* Installing `statsmodels`:
    + https://www.statsmodels.org/stable/install.html

* `Rdatasets` repository and `statsmodels` datasets:
    + https://github.com/vincentarelbundock/Rdatasets/blob/master/datasets.csv
    + https://cran.r-project.org/web/packages/causaldata/causaldata.pdf
    + https://www.statsmodels.org/stable/datasets/index.html
    + https://www.statsmodels.org/stable/datasets/generated/sunspots.html

* Getting help with `statsmodels`:
    + https://www.statsmodels.org/stable/generated/statsmodels.tools.web.webdoc.html#statsmodels.tools.web.webdoc
    + https://www.statsmodels.org/stable/endog_exog.html

* Loading data, model fit, and summary procedure:
    + https://www.statsmodels.org/stable/gettingstarted.html

* Generalized Linear Models:
    + https://sscc.wisc.edu/sscc/pubs/glm-r/
    + https://online.stat.psu.edu/stat504/lesson/6/6.1
    + https://www.mygreatlearning.com/blog/generalized-linear-models/
    + https://www.statsmodels.org/stable/examples/notebooks/generated/glm.html

* Logistic Regression:
    + https://www.andrewvillazon.com/logistic-regression-python-statsmodels/

* Poisson Regression:
    + https://tidypython.com/poisson-regression-in-python/

* Non-parametric Methods:
    + https://mathisonian.github.io/kde/
    + https://www.statsmodels.org/stable/nonparametric.html
    + https://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kde.KDEUnivariate.html
    + https://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kernel_density.KDEMultivariate.html
    + https://www.statsmodels.org/stable/examples/notebooks/generated/kernel_density.html 

* Diagnostic tests:
    + https://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests

* NYC 311 Service Request Data:
    + https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data

