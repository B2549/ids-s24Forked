---
title: "Hypothesis Testing with scipy.stats"
author: "Isabelle Perez"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: default
--- 
## Introduction 

## Basic Statistical Hypothesis Tests 
### Two-sample t-test 
* Assumes observations are independent and identically distributed (i.i.d), 
normally distributed, and have equal variances. 
* Takes parameter alternative that can be set to "two_sided", "less", or "greater". 
* Takes optional parameter for handling null values. 
  + Can "propagate" (return NaN), "raise" (raise `ValueError`), or "omit"(ignore null values). 
* Takes optional boolean parameter indicating equal variances. 
  + Default is True.  
  + If set to False, performs Welch's t-test, which doesn't use pooled sample variance. 

| H~0~: $\mu_1 = \mu_2$, where $\mu_i$ is the population mean of population $i$  
| H~1~: $\mu_1 \neq$ or $>$ or $<$ $\mu_2$ 

Looking at the 2022-2023 rodent sighting data from the NYC 311 Service Requests, there are many
ways a two-sample t-test may be useful. For example, we can consider samples drawn from different
boroughs and perform this hypothesis test to identify whether their mean response time to 
the service requests differ. 
```{python} 
import pandas as pd  
import scipy.stats  

# read in file 
df = pd.read_csv('rodent_2022-2023.csv')  

# data cleaning - change dates to timestamp object
df['Created Date'] = pd.to_datetime(df['Created Date'])
df['Closed Date'] = pd.to_datetime(df['Closed Date'])
df['Response Time'] = df['Closed Date'] - df['Created Date']

# convert data to total seconds
df['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds()) 

# select Bronx and Queens boroughs 
df_bronx = df.loc[df['Borough'] == 'BRONX']
df_queens = df.loc[df['Borough'] == 'QUEENS']

# sample from respective dfs 
sample1 = df_bronx.sample(1000, random_state = 0)['Response Time']
sample2 = df_queens.sample(1000, random_state = 0)['Response Time'] 

# show the samples have unequal variances 
print(sample1.std())
print(sample2.std())
```
Since the two samples have very different standard deviations, and therefore unequal variances, 
we must run the test without a pooled sample variance, by setting `equal_var` equal to `False`.

```{python}
# perform test and omit null values
t_1, p_1 = scipy.stats.ttest_ind(sample1, sample2, equal_var = False, nan_policy = 'omit')

# perform test and propagate null values
t_2, p_2 = scipy.stats.ttest_ind(sample1, sample2, equal_var = False, nan_policy = 'propagate')

# perform one-sided test (mean sample 1 > mean sample 2)
t_3, p_3 = scipy.stats.ttest_ind(sample1, sample2, equal_var = False, alternative = 'greater', nan_policy = 'omit')

# print results 
print(t_1, p_1)
print(t_2, p_2)
print(t_3, p_3)
``` 
As per the above results, at most confidence levels, the p-value of $1.23 * 10^-15$ will lead to 
a rejection of the null hypothesis, suggesting that the mean response times for the Bronx and
Queens differ significantly. When the null values are propagated instead of omitted, though,
the t-statistic and p-value both return `nan` due to missing values. And in the third case, when 
the alterative is set to $>$, the null hypothesis would not be rejected at most confidence levels,
suggesting that although the mean response time for the Bronx and Queens differ, that for the Bronx
is not significantly larger.  

### Paired t-test 
* Same assumptions as t-test with addition of observations across the two samples being 
paired. 
* Same null and alternate hypotheses as t-test and same parameters.  
* The input arrays must have the same shape. 

| H~0~: $\mu_1 = \mu_2$
| H~1~: $\mu_1 \neq$ or $>$ or $<$ $\mu_2$   

## Normality 
### Shapiro-Wilk Test 
* Used to test normality of a data sample.
* Assumes observations are i.i.d. 

| H~0~: data is drawn from a normal distribution  
| H~1~: data is not drawn from a normal distribution   

```{python}  
# only select rows where Response Time is not null 
modified_df = df[df['Response Time'].notnull()] 

sample3 = modified_df.sample(5000, random_state = 0)['Response Time'] 

t_4, p_4 = scipy.stats.shapiro(sample3)

print(t_4, p_4)  
```  

```{python}
import matplotlib.pyplot as plt 
 
bins = [i for i in range(0, 12600000, 1000000)]
 
plt.xticks(bins)

# set axis labels 
plt.xlabel('Response Time') 
plt.ylabel('Value Counts')

# plot sample3 data 
plt.hist(sample3.values, bins = bins) 
``` 

### NormalTest 
* Based on the D'Agostino-Pearson test for assessing normality with shape. 
  + Measures skewness and kurtosis, which is the heaviness/lightness of the tail. 
  + A normal distribution has skewness of $0$ and kurtosis of $3$.  
* Assumes observations are i.i.d. 

| H~0~: data is drawn from a normal distribution  
| H~1~: data is not drawn from a normal distribution   

```{python}
t_5, p_5 = scipy.stats.normaltest(sample3)

print(t_5, p_5)  
``` 

## Correlation   
### Pearson's Correlation 
* Tests for a linear relationship between two samples.
* Assumes observations are i.i.d, normally distributed, and have equal variances.  
* Returns a correlation coefficient as well as the p-value corresponding to the chosen 
hypothesis. 
* Raises `ConstantInputWarning` if an input array has all constant values.   

| H~0~: the samples are independent 
| H~1~: there exists dependency between the samples 

```{python}
# only select rows where neither Response Time nor Latitude is null 
modified_df2 = modified_df[modified_df['Latitude'].notnull()]

sample4 = modified_df2.sample(5000, random_state = 0)  

x = sample4['Latitude'] 
y = sample4['Response Time'] 

t_6, p_6 = scipy.stats.pearsonr(x, y)

print(t_6, p_6)
``` 

## Nonparametric Hypothesis Tests   
### Mann-Whitney U Test 
* Tests that two samples are drawn from the same distribution. 
* Assumes observations are i.i.d. and can be ranked/compared. 
* Nonparametric version of t-test. 
* Takes optional parameter `method` with three options. 
  + `asymptotic` provides an estimation of the test statistic using sum
  of rankings.  
  + `exact` calculates the exact p-value. 
  + Default option `auto` uses `exact` for sample sizes $<= 8$ and 
  `asymptotic` otherwise. 

| H~0~: distribution of sample 1 $=$ distribution of sample 2
| H~1~: distribution of sample 1 $\neq$ or $>$ or $<$ distribution of sample 2  

```{python}
t_7, p_7 = scipy.stats.mannwhitneyu(sample1, sample2, nan_policy = 'omit')

print(t_7, p_7)
```   