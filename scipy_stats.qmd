---
title: "Hypothesis Testing with scipy.stats"
author: "Isabelle Perez"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: default
--- 
## Introduction 

## Overview of `scipy.stats` 
The package `scipy.stats`  

## Basic Statistical Hypothesis Tests 
### Two-sample t-test 

| H~0~: $\mu_1 = \mu_2$  
| H~1~: $\mu_1 \neq$ or $>$ or $<$ $\mu_2$  

**Code:** `scipy.stats.ttest_ind(sample1, sample2)`  

**Assumptions:** Observations are independent and identically distributed (i.i.d), 
normally distributed, and have equal variances. 

**Optional Parameters:**

* `nan_policy` can be set to `propagate` (return `nan`), `raise` (raise `ValueError`),
or `omit`(ignore null values).
* `alternative` can be `two-sided` (default), `less`, or `greater`. 
* `equal_var` is a boolean representing whether the variances of the two samples are 
equal. 
* `axis` defines the axis along which the statistic should be computed (default is 0). 

**Returns:** The t-statisic, a corresponding p-value, and the degrees of freedom. 

### Paired t-test 

| H~0~: $\mu_1 = \mu_2$
| H~1~: $\mu_1 \neq$ or $>$ or $<$ $\mu_2$  
 
**Code:** `scipy.stats.ttest_rel(sample1, sample2)` 

**Assumptions:** Observations are independent and identically distributed (i.i.d), 
normally distributed, have equal variances, and are related. The input arrays must
also be of the same size since the observations are paired.  

**Optional Parameters:** Can use `nan_policy` or `alternative`. 

**Returns:** The t-statisic, a corresponding p-value, and the degrees of freedom. 
Also has a method called `confidence_interval` with input parameter `confidence_level`
that returns a tuple with the confidence interval around the difference in 
population means of the two samples.  

### ANOVA 

| H~0~: $\mu_1 = \mu_2 = ... = \mu_n$
| H~1~: at least two $\mu_i$ are not equal  

**Code:** `scipy.stats.f_oneway(sample1, sample2, ..., samplen)`

### Example: Comparison of Mean Response Times by Borough  
Looking at the 2022-2023 rodent sighting data from the NYC 311 Service Requests, there are many
ways a two-sample t-test may be useful. For example, we can consider samples drawn from different
boroughs and perform this hypothesis test to identify whether their mean response time to 
the service requests differ. 
```{python} 
import pandas as pd  
import scipy.stats  

# read in file 
df = pd.read_csv('rodent_2022-2023.csv')  

# data cleaning - change dates to timestamp object
df['Created Date'] = pd.to_datetime(df['Created Date'])
df['Closed Date'] = pd.to_datetime(df['Closed Date'])

# add column Response Time 
df['Response Time'] = df['Closed Date'] - df['Created Date']

# convert data to total seconds
df['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600)    
``` 

```{python}
# select Bronx and Queens boroughs 
df_man = df.loc[df['Borough'] == 'MANHATTAN']['Response Time']
df_queens = df.loc[df['Borough'] == 'QUEENS']['Response Time'] 

# show the samples have unequal variances 
print('Mean, variance for Manhattan:', (df_man.mean(), df_man.std() ** 2))
print('Mean, variance for Queens:', (df_queens.mean(), df_queens.std() ** 2)) 
```
Since the two samples have very different variances, we need to run the test without a 
pooled sample variance, by setting `equal_var` equal to `False`.

```{python}

``` 
As per the above results, at most confidence levels, the p-value of $1.23 \times 10^{-15}$ will lead to 
a rejection of the null hypothesis, suggesting that the mean response times for the Bronx and
Queens differ significantly. When the null values are propagated instead of omitted, though,
the t-statistic and p-value both return `nan` due to missing values. And in the third case, when 
the alterative is set to $>$, the null hypothesis would not be rejected at most confidence levels,
suggesting that although the mean response time for the Bronx and Queens differ, that for the Bronx
is not significantly larger.   

## Normality 
### Shapiro-Wilk Test 
* Used to test normality of a data sample.
* Assumes observations are i.i.d. 

| H~0~: data is drawn from a normal distribution  
| H~1~: data is not drawn from a normal distribution   

In order to describe data, it may be useful to identify whether the data is normally distributed
or not. We can take a sample from the data frame and test whether the distribution of response 
times is normal or not. The Shapiro-Wilk test does not have options to omit null values,
so the sample must be selected from rows where the Response Time is not null. 
```{python}  
# only select rows where Response Time is not null 
modified_df = df[df['Response Time'].notnull()] 

sample3 = modified_df.sample(5000, random_state = 0)['Response Time'] 

# perform test  
t_4, p_4 = scipy.stats.shapiro(sample3) 

print(t_4, p_4)  
```  
With such a small p-value, we can reject the null hypothesis at most confidence levels and
conclude that the Response Times for the rodent sighting service requests are not
normally distributed. 

We can also graph the sample using `matplotlib` to see that the data is not normally distributed. 
```{python}
import matplotlib.pyplot as plt 
 
bins = [i for i in range(0, 2500, 200)] 
 
plt.xticks(bins)

# set axis labels 
plt.xlabel('Response Time') 
plt.ylabel('Value Counts')

# plot sample3 data 
plt.hist(sample3.values, bins = bins) 
``` 

### NormalTest 
* Based on the D'Agostino-Pearson test for assessing normality with shape. 
  + Measures skewness and kurtosis, which is the heaviness/lightness of the tail. 
  + A normal distribution has skewness of $0$ and kurtosis of $3$.  
* Assumes observations are i.i.d. 

| H~0~: data is drawn from a normal distribution  
| H~1~: data is not drawn from a normal distribution   

```{python}
t_5, p_5 = scipy.stats.normaltest(sample3)

print(t_5, p_5)  
``` 

## Correlation   
### Pearson's Correlation 
* Tests for a linear relationship between two samples.
* Assumes observations are i.i.d, normally distributed, and have equal variances.  
* Returns a correlation coefficient as well as the p-value corresponding to the chosen 
hypothesis. 
* Raises `ConstantInputWarning` if an input array has all constant values.   

| H~0~: the samples are independent 
| H~1~: there exists dependency between the samples 

```{python}
# only select rows where neither Response Time nor Latitude is null 
modified_df2 = modified_df[modified_df['Latitude'].notnull()]

sample4 = modified_df2.sample(5000, random_state = 0)  

x = sample4['Latitude'] 
y = sample4['Response Time'] 

t_6, p_6 = scipy.stats.pearsonr(x, y)

print(t_6, p_6)
``` 

## Nonparametric Hypothesis Tests   
### Mann-Whitney U Test 
* Tests that two samples are drawn from the same distribution. 
* Assumes observations are i.i.d. and can be ranked/compared. 
* Nonparametric version of t-test. 
* Takes optional parameter `method` with three options. 
  + `asymptotic` provides an estimation of the test statistic using sum
  of rankings.  
  + `exact` calculates the exact p-value. 
  + Default option `auto` uses `exact` for sample sizes $<= 8$ and 
  `asymptotic` otherwise. 

| H~0~: distribution of sample 1 $=$ distribution of sample 2
| H~1~: distribution of sample 1 $\neq$ or $>$ or $<$ distribution of sample 2  

```{python}

```   