---
title: "Hypothesis Testing with scipy.stats"
author: "Isabelle Perez"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: default
--- 
## Introduction 

## Overview of `scipy.stats` 
The package `scipy.stats`  

## Basic Statistical Hypothesis Tests 
### Two-sample t-test 

| H~0~: $\mu_1 = \mu_2$  
| H~1~: $\mu_1 \neq$ or $>$ or $<$ $\mu_2$   

**Code:** `scipy.stats.ttest_ind(sample_1, sample_2)`  

**Assumptions:** Observations are independent and identically distributed (i.i.d), 
normally distributed, and the two samples have equal variances. 

**Optional Parameters:**

* `nan_policy` can be set to `propagate` (return `nan`), `raise` (raise `ValueError`),
or `omit`(ignore null values).
* `alternative` can be `two-sided` (default), `less`, or `greater`. 
* `equal_var` is a boolean representing whether the variances of the two samples are 
equal (default is True). 
* `axis` defines the axis along which the statistic should be computed (default is 0). 

**Returns:** The t-statisic, a corresponding p-value, and the degrees of freedom. 

### Paired t-test 

| H~0~: $\mu_1 = \mu_2$
| H~1~: $\mu_1 \neq$ or $>$ or $<$ $\mu_2$  
 
**Code:** `scipy.stats.ttest_rel(sample_1, sample_2)` 

**Assumptions:** Observations are i.i.d, normally distributed, and related, 
and the two samples have equal variances. The input arrays must also be 
of the same size since the observations are paired.  

**Optional Parameters:** Can use `nan_policy` or `alternative`. 

**Returns:** The t-statisic, a corresponding p-value, and the degrees of freedom. 
Also has a method called `confidence_interval` with input parameter `confidence_level`
that returns a tuple with the confidence interval around the difference in 
population means of the two samples.  

### ANOVA 

| H~0~: $\mu_1 = \mu_2 = ... = \mu_n$
| H~1~: at least two $\mu_i$ are not equal  

**Code:** `scipy.stats.f_oneway(sample_1, sample_2, ..., sample_n)`

**Assumptions:** Samples are i.i.d., normally distributed, and the samples have
equal variances. 

**Errors:** 

* Raises `ConstantInputWarning` if all values in each of the inputs are 
identical. 
* Raises `DegenerateDataWarning` if any input has length $0$ or all inputs
have length $1$. 

**Returns:** The F-statistic and a corresponding p-value. 

### Example: Comparison of Mean Response Times by Borough  
Looking at the 2022-2023 rodent sighting data from the NYC 311 Service Requests, there are many
ways a two-sample t-test may be useful. For example, we can consider samples drawn from different
boroughs and perform this hypothesis test to identify whether their mean response times differ. 
If so, this may suggest that some boroughs are being underserviced.  
```{python} 
import pandas as pd 
import numpy as np 
import scipy.stats   

# read in file 
df = pd.read_csv('rodent_2022-2023.csv')  

# data cleaning - change dates to timestamp object
df['Created Date'] = pd.to_datetime(df['Created Date'])
df['Closed Date'] = pd.to_datetime(df['Closed Date'])

# add column Response Time 
df['Response Time'] = df['Closed Date'] - df['Created Date']

# convert data to total seconds
df['Response Time'] = df['Response Time'].apply(lambda x: x.total_seconds() / 3600)    
``` 

Since the two-sample t-test assumes the data is drawn from a normal distribution, we need to
ensure the samples we are comparing are normally distributed. According to the Central Limit theorem,
the distribution of sample means from repeated samples of a population will be roughly normal. 
Therefore, we can take 100 samples of each borough's response times, measure the mean of each sample,
and perform the hypothesis test on the arrays of sample means. 
```{python}
import matplotlib.pyplot as plt 

# select Bronx and Queens boroughs 
df_mhtn = df[df['Borough'] == 'MANHATTAN']['Response Time'] 
df_queens = df[df['Borough'] == 'QUEENS']['Response Time']  

mhtn_means = []
queens_means = []

# create samples of sampling means 
for i in range(100): 
  sample1 = df_mhtn.sample(1000, replace = True)
  mhtn_means.append(sample1.mean())

  sample2 = df_queens.sample(1000, replace = True) 
  queens_means.append(sample2.mean())  

# plot distribution of sample means for Manhattan
plt.hist(mhtn_means)
plt.xlabel('Mean Response Times for Manhattan')
plt.ylabel('Value Counts')
plt.show()

# plot distribution of sample means for Queens 
plt.hist(queens_means) 
plt.xlabel('Mean Response Times for Queens')
plt.ylabel('Value Counts')
plt.show() 
```  

We also need to check if the variances of the two samples are equal. 
```{python}
# convert to numpy array 
mhtn_means = np.array(mhtn_means)
queens_means = np.array(queens_means)

print('Mean, variance for Manhattan', (mhtn_means.mean(), mhtn_means.std() ** 2))
print('Mean, variance for Queens:', (queens_means.mean(), queens_means.std() ** 2))
```
Since the ratio of the variances is less than $2$, it is safe to assume equal variances.  

```{python}
result_1 = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True)

print('t-statistic:', result_1.statistic)
print('p-value:', result_1.pvalue) 
print('degrees of freedom:', result_1.df) 
``` 
At an alpha level of $0.05$, the p-value allows us to reject the null hypothesis
and conclude that there is a statistically significant difference in the mean of 
sample means drawn from rodent sighting response times for Manhattan compared to Queens. 

```{python}
result_2 = scipy.stats.ttest_ind(mhtn_means, queens_means, equal_var = True, alternative = 'less') 

print('t-statistic:', result_2.statistic)
print('p-value:', result_2.pvalue) 
print('degrees of freedom:', result_2.df) 
```
We can also set the alternative equal to `less` to test if the mean of sample means
drawn from the Manhattan response times is less than that of sample means drawn from
Queens response times. At the alpha level of $0.05$, we can also reject this null
hypothesis and conclude that the mean of sample means is less for Manhattan than 
it is for Queens. 

## Normality 
### Shapiro-Wilk Test 

| H~0~: data is drawn from a normal distribution  
| H~1~: data is not drawn from a normal distribution     

**Code:** `scipy.stats.shapiro(sample)` 

**Assumptions:** Observations are i.i.d. 

**Returns:** The test statistic and corresponding p-value. 

* More appropriate for smaller sample sizes ($<50$). 
* The closer the test statistic is to $1$, the closer it is to a normal 
distribution, with $1$ being a perfect match.   

### NormalTest 

| H~0~: data is drawn from a normal distribution  
| H~1~: data is not drawn from a normal distribution   



## Correlation   
### Pearson's Correlation 
* Tests for a linear relationship between two samples.
* Assumes observations are i.i.d, normally distributed, and have equal variances.  
* Returns a correlation coefficient as well as the p-value corresponding to the chosen 
hypothesis. 
* Raises `ConstantInputWarning` if an input array has all constant values.   

| H~0~: the samples are independent 
| H~1~: there exists dependency between the samples 

## Nonparametric Hypothesis Tests   
### Mann-Whitney U Test 
* Tests that two samples are drawn from the same distribution. 
* Assumes observations are i.i.d. and can be ranked/compared. 
* Nonparametric version of t-test. 
* Takes optional parameter `method` with three options. 
  + `asymptotic` provides an estimation of the test statistic using sum
  of rankings.  
  + `exact` calculates the exact p-value. 
  + Default option `auto` uses `exact` for sample sizes $<= 8$ and 
  `asymptotic` otherwise. 

| H~0~: distribution of sample 1 $=$ distribution of sample 2
| H~1~: distribution of sample 1 $\neq$ or $>$ or $<$ distribution of sample 2   